# PRD: Tesla K80 Support for vLLM

## Executive Summary

Add support for Tesla K80 GPUs (compute capability 3.7) to vLLM through a specialized Docker-based build and runtime environment. This will enable running vLLM on legacy hardware that is still widely available but currently unsupported.

## Background

Tesla K80 is a legacy GPU with compute capability 3.7 that is incompatible with modern vLLM due to:
- Minimum compute capability requirement of 7.0 in current vLLM
- Dependency on modern CUDA toolkit (12.x) incompatible with K80
- Advanced features requiring newer GPU architectures

## Goals

### Primary Objectives
- Enable vLLM inference on Tesla K80 hardware
- Maintain compatibility with basic LLM inference workloads
- Provide Docker-based solution for isolated environment management

### Success Metrics
- vLLM successfully loads and runs inference on K80
- Performance benchmarks comparable to other legacy GPU solutions
- Stable operation with common LLM models (7B-13B parameter range)

## Technical Requirements

### Hardware Constraints
- **GPU**: Tesla K80 (compute capability 3.7)
- **CUDA**: Version 11.4 (last version supporting K80)
- **Driver**: Legacy NVIDIA driver compatible with CUDA 11.4
- **Memory**: 12GB GDDR5 per K80 card

### Software Architecture

#### Docker Environment
- Base image: Ubuntu 20.04 with CUDA 11.4 runtime
- PyTorch version compatible with CUDA 11.4
- Isolated build environment preventing conflicts with host system

#### Code Modifications Required

1. **Platform Support**
   - Modify `vllm/platforms/cuda.py` to recognize capability 3.7
   - Update dtype support for K80 (FP32 primary, limited FP16)
   - Adjust memory allocation strategies

2. **Attention Backend**
   - Force XFormers backend (disable FlashAttention)
   - Implement capability-aware backend selection
   - Optimize for K80's memory bandwidth limitations

3. **Feature Compatibility**
   - Disable quantization methods requiring modern GPUs
   - Disable advanced kernels (BitBlas, Marlin, etc.)
   - Fallback implementations for unsupported operations

4. **Build System**
   - CMAKE configuration for CUDA 11.4
   - Conditional compilation flags for legacy support
   - Custom wheel build process

## Implementation Plan

### Phase 1: Docker Environment (Week 1) ‚úÖ COMPLETED
- ‚úÖ Create Dockerfile with CUDA 11.4.3 base (Rocky Linux 8)
- ‚úÖ Set up build environment with compatible dependencies
- ‚úÖ Create docker-compose configuration for production and development
- ‚úÖ Validate CUDA runtime functionality
- ‚úÖ Comprehensive README with setup instructions

**Files Created:**
- `docker/Dockerfile.tesla-k80`
- `docker/docker-compose.k80.yml` 
- `docker/README-tesla-k80.md`

### Phase 2: Core Compatibility (Week 2) ‚úÖ COMPLETED
**Platform Detection Updates:**
- [x] Modify `vllm/platforms/cuda.py` to recognize compute capability 3.7
- [x] Update supported_dtypes to include K80 (FP32 primary, limited FP16)
- [x] Add K80-specific memory allocation strategies (basic implementation)
- [x] Update documentation strings and comments

**Attention Backend Selection:**
- [x] Force XFormers backend for capability 3.7 in `get_attn_backend_cls()`
- [x] Disable FlashAttention for K80 (requires capability ‚â•8.0)
- [x] Add fallback logic in attention selector for Kepler architecture
- [x] Update backend compatibility checks for both V0 and V1 engines

**Kernel Optimizations:**
- [x] Updated prefix_prefill.py with smaller BASE_BLOCK (32) for K80
- [x] Added IS_KEPLER flag for architecture-specific checks
- [x] Reduced register pressure for older GPU architecture

**Worker Initialization:**
- [x] Platform-level dtype validation handles K80 correctly
- [x] Both V0 and V1 workers properly detect and use XFormers for K80
- [x] Error handling improved with clear messaging for unsupported features

### Phase 3: Feature Adaptation (Week 3) ‚úÖ PARTIALLY COMPLETED
**Quantization Disabling:**
- [x] Disable BitBlas quantization (requires capability ‚â•7.0) - Added documentation
- [x] Disable Marlin FP8/FP4 quantization (requires capability ‚â•8.0) - Added documentation  
- [ ] Disable NVFP4 quantization (requires capability 10.0)
- [x] Update quantization method detection logic - Capability checks working

**Kernel Compatibility:**
- [ ] Disable advanced CUTLASS kernels for K80
- [ ] Force basic CUDA kernels where needed
- [ ] Update MOE kernel selection for capability 3.7
- [ ] Implement fallback implementations

**Memory Optimization:**
- [x] Adjust block size calculations for K80's memory bandwidth - Basic implementation in prefix_prefill
- [ ] Update cache allocation strategies
- [ ] Optimize for GDDR5 memory characteristics

### Phase 4: Build System Updates (Week 3) üìã PLANNED
**CMake Configuration:**
- [ ] Update CMakeLists.txt for CUDA 11.4 compatibility
- [ ] Add compute capability 3.7 to build targets
- [ ] Configure appropriate compiler flags
- [ ] Update external dependencies (CUTLASS, etc.)

**Python Setup:**
- [ ] Modify setup.py to handle K80 builds
- [ ] Add environment variable detection
- [ ] Update wheel building for K80 support
- [ ] Configure conditional compilation

### Phase 5: Testing & Validation (Week 4) üìã PLANNED
**Unit Testing:**
- [ ] Create K80-specific test cases
- [ ] Test platform detection logic
- [ ] Validate attention backend selection
- [ ] Test memory allocation

**Integration Testing:**
- [ ] Test with small models (DialoGPT-small, GPT2)
- [ ] Validate Docker build process
- [ ] Test API server functionality
- [ ] Performance benchmarking

**Documentation:**
- [x] Update main README with K80 support - Docker README completed  
- [x] Create troubleshooting guide - Comprehensive README with troubleshooting
- [x] Document performance expectations - Added to README
- [ ] Add usage examples
- [ ] Update main vLLM README with Tesla K80 support mention

## Next Steps Recommendation

### üéØ **Immediate Priority: Build System (Phase 4)**
The core compatibility layer is complete. Next critical step:

1. **CMakeLists.txt Updates** - Add CUDA 11.4 and compute capability 3.7 support
2. **setup.py Modifications** - Handle K80-specific build requirements  
3. **Docker Build Testing** - Validate the complete build process

### üß™ **Testing Phase (Phase 5)**  
Once build system is updated:

1. **Docker Build Validation** - Test complete Docker build process
2. **Small Model Testing** - DialoGPT-small, GPT2 inference testing
3. **Performance Benchmarking** - Document actual K80 performance
4. **Error Handling Validation** - Ensure graceful failures for unsupported features

### üìã **Current Implementation Status**
- **Phase 1**: ‚úÖ Complete (Docker environment)
- **Phase 2**: ‚úÖ Complete (Core compatibility)  
- **Phase 3**: üîÑ Partially complete (Feature adaptation)
- **Phase 4**: ‚è∏Ô∏è Pending (Build system)
- **Phase 5**: ‚è∏Ô∏è Pending (Testing)

## Technical Risks

### High Risk
- **CUDA Compatibility**: CUDA 11.4 may have compatibility issues with modern PyTorch
- **Performance**: K80's old architecture may severely limit inference speed
- **Memory Bandwidth**: Limited memory bandwidth may create bottlenecks

### Medium Risk
- **Kernel Compatibility**: Some CUDA kernels may not compile for compute 3.7
- **Driver Issues**: Legacy driver compatibility with modern Linux distributions
- **Maintenance Burden**: Supporting legacy hardware increases complexity

### Mitigation Strategies
- Comprehensive Docker isolation to prevent system conflicts
- Conservative feature set focusing on basic inference only
- Clear documentation of limitations and unsupported features

## Constraints & Limitations

### Supported Features
- Basic text generation inference
- Standard attention mechanisms (no FlashAttention)
- FP32 and limited FP16 precision
- Single-GPU inference only

### Unsupported Features
- Quantization (INT8, FP8, etc.)
- Advanced kernels (Marlin, BitBlas)
- Multi-GPU parallelism
- Modern attention optimizations
- Speculative decoding

### Performance Expectations
- 2-5x slower than modern GPUs
- Memory-bound operations due to GDDR5 limitations
- Suitable for experimentation and development, not production

## Resource Requirements

### Development
- Tesla K80 hardware for testing
- Development time: ~4 weeks
- Docker registry space for custom images

### Runtime
- Docker environment (2-3GB image size)
- Legacy NVIDIA driver installation
- Increased memory usage due to fallback implementations

## Success Criteria

1. **Functional**: vLLM loads and runs inference on K80
2. **Performance**: Achieves at least 1-2 tokens/second for 7B models
3. **Stability**: Runs continuously without memory leaks or crashes
4. **Documentation**: Clear setup and usage instructions
5. **Compatibility**: Works with popular open-source models

## Future Considerations

- Support for other Kepler architecture GPUs (K40, etc.)
- Optimization for specific model architectures
- Integration with model quantization for better performance
- Community adoption and feedback integration