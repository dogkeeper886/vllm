# vLLM K80 Runtime Image (Local source)
# Copies local vLLM source and builds against the builder image.
# Use this during development to avoid re-cloning from GitHub.
#
# Requires: vllm37-builder image (built from docker/k80/builder/Dockerfile)
#
# Build (from repo root):
#   docker build -t vllm37-local \
#     -f docker/k80/runtime/Dockerfile.local .

ARG BUILDER_IMAGE=vllm37-builder
FROM ${BUILDER_IMAGE}

ARG MAX_JOBS=4

# ============================================================================
# Copy local source
# ============================================================================
COPY . /usr/local/src/vllm37

WORKDIR /usr/local/src/vllm37

# Install build system requirements
RUN pip install --no-cache-dir "setuptools>=77.0.3,<80.0.0" "setuptools-scm>=8.0"

# Install Python dependencies (use K80-specific requirements)
RUN pip install --no-cache-dir -r requirements/cuda_k80.txt 2>/dev/null || \
    pip install --no-cache-dir -r requirements/common.txt

# Build and install vLLM with legacy CUDA flags
# --no-deps prevents pip from pulling torch 2.7.1 which would overwrite
# our custom PyTorch 2.0.1 built with CUDA 11.4 + sm_37 support.
RUN CUDA_HOME=/usr/local/cuda-11.4 \
    TORCH_CUDA_ARCH_LIST="3.7" \
    VLLM_BUILD_LEGACY_CUDA=1 \
    CMAKE_POLICY_VERSION_MINIMUM=3.5 \
    MAX_JOBS=${MAX_JOBS} \
    pip install . --no-build-isolation --no-deps

# Downgrade numpy to 1.x (PyTorch 2.0.1 was compiled against numpy 1.x)
RUN pip install --no-cache-dir "numpy<2"

# Switch WORKDIR away from source tree so Python doesn't shadow the
# installed package (site-packages) with the source directory.
WORKDIR /workspace

# ============================================================================
# Runtime configuration
# ============================================================================
# NVIDIA Container Toolkit requires these to expose GPUs inside the container
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility
ENV VLLM_USE_V1=0
ENV CUDA_HOME=/usr/local/cuda-11.4
ENV VLLM_ATTENTION_BACKEND=TORCH_SDPA
# PyTorch 2.0 dynamo has broken SymInt handling — disable entirely
ENV TORCHDYNAMO_DISABLE=1
# K80 is PCIe-only (no NVLink) — P2P causes kernel timeouts with TP>1
ENV NCCL_P2P_DISABLE=1
# Safer multiprocessing for PyTorch 2.0 distributed
ENV VLLM_WORKER_MULTIPROC_METHOD=spawn

EXPOSE 8000

ENTRYPOINT ["python", "-m", "vllm.entrypoints.openai.api_server"]
CMD ["--model", "TinyLlama/TinyLlama-1.1B-Chat-v1.0", \
     "--dtype", "float32", \
     "--enforce-eager", \
     "--tensor-parallel-size", "4", \
     "--max-model-len", "2048", \
     "--gpu-memory-utilization", "0.85", \
     "--swap-space", "0"]
